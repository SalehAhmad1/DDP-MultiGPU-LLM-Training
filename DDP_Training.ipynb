{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "806aad4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pandas -qq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fc527a0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['Question', 'Answer'],\n",
       "    num_rows: 30\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "raw = load_dataset(\"csv\", data_files='agriculture_qa_csv.csv')['train']\n",
    "raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "71a9d56f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Question', 'Answer']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw.column_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73371428",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, List\n",
    "\n",
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    set_seed,\n",
    ")\n",
    "from datasets import load_dataset\n",
    "from peft import LoraConfig, get_peft_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d5d9ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_args():\n",
    "    p = argparse.ArgumentParser(description=\"LoRA fine-tune a small LLM on QA CSV (multi-GPU)\")\n",
    "    p.add_argument(\"--data_csv\", type=str, required=True, help=\"Path to CSV with columns Question,Answer\")\n",
    "    p.add_argument(\"--output_dir\", type=str, default=\"./checkpoints/tinyllama_qa\", help=\"Where to save checkpoints\")\n",
    "    p.add_argument(\n",
    "        \"--base_model\",\n",
    "        type=str,\n",
    "        default=\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\n",
    "        help=(\n",
    "            \"Hugging Face model id. Examples: \\n\"\n",
    "            \"  TinyLlama/TinyLlama-1.1B-Chat-v1.0 (default),\\n\"\n",
    "            \"  EleutherAI/pythia-70m-deduped,\\n\"\n",
    "            \"  sshleifer/tiny-gpt2,\\n\"\n",
    "            \"  facebook/opt-350m\"\n",
    "        ),\n",
    "    )\n",
    "    p.add_argument(\"--seed\", type=int, default=42)\n",
    "    p.add_argument(\"--max_length\", type=int, default=768, help=\"Max sequence length\")\n",
    "    p.add_argument(\"--num_train_epochs\", type=float, default=3.0)\n",
    "    p.add_argument(\"--learning_rate\", type=float, default=2e-4)\n",
    "    p.add_argument(\"--warmup_ratio\", type=float, default=0.03)\n",
    "    p.add_argument(\"--weight_decay\", type=float, default=0.0)\n",
    "    p.add_argument(\"--per_device_train_batch_size\", type=int, default=2)\n",
    "    p.add_argument(\"--per_device_eval_batch_size\", type=int, default=2)\n",
    "    p.add_argument(\"--gradient_accumulation_steps\", type=int, default=8)\n",
    "    p.add_argument(\"--logging_steps\", type=int, default=50)\n",
    "    p.add_argument(\"--save_steps\", type=int, default=200)\n",
    "    p.add_argument(\"--eval_ratio\", type=float, default=0.05, help=\"Fraction for validation split\")\n",
    "    p.add_argument(\"--gradient_checkpointing\", action=\"store_true\", help=\"Enable gradient checkpointing\")\n",
    "    p.add_argument(\"--resume_from_checkpoint\", type=str, default=None)\n",
    "    # LoRA params\n",
    "    p.add_argument(\"--lora_r\", type=int, default=16)\n",
    "    p.add_argument(\"--lora_alpha\", type=int, default=32)\n",
    "    p.add_argument(\"--lora_dropout\", type=float, default=0.05)\n",
    "    p.add_argument(\n",
    "        \"--target_modules\",\n",
    "        type=str,\n",
    "        default=\"q_proj,k_proj,v_proj,o_proj,gate_proj,up_proj,down_proj\",\n",
    "        help=\"Comma-separated list of target modules for LoRA\",\n",
    "    )\n",
    "\n",
    "    return p.parse_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d389211",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_trainable_params(model: torch.nn.Module):\n",
    "    trainable, total = 0, 0\n",
    "    for _, p in model.named_parameters():\n",
    "        num = p.numel()\n",
    "        total += num\n",
    "        if p.requires_grad:\n",
    "            trainable += num\n",
    "    if torch.distributed.is_initialized():\n",
    "        if torch.distributed.get_rank() == 0:\n",
    "            print(f\"Trainable params: {trainable:,} / {total:,} ({100*trainable/total:.2f}%)\")\n",
    "    else:\n",
    "        print(f\"Trainable params: {trainable:,} / {total:,} ({100*trainable/total:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6c411d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class QADatasetCollator:\n",
    "    tokenizer: AutoTokenizer\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, List[int]]]) -> Dict[str, torch.Tensor]:\n",
    "        input_ids = [torch.tensor(f[\"input_ids\"], dtype=torch.long) for f in features]\n",
    "        labels = [torch.tensor(f[\"labels\"], dtype=torch.long) for f in features]\n",
    "        attention = [torch.ones_like(i) for i in input_ids]\n",
    "\n",
    "        input_ids = pad_sequence(input_ids, batch_first=True, padding_value=self.tokenizer.pad_token_id)\n",
    "        labels = pad_sequence(labels, batch_first=True, padding_value=-100)\n",
    "        attention = pad_sequence(attention, batch_first=True, padding_value=0)\n",
    "\n",
    "        return {\"input_ids\": input_ids, \"attention_mask\": attention, \"labels\": labels}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4166e74a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_prompt_and_labels(tokenizer: AutoTokenizer, q: str, a: str, max_length: int) -> Dict[str, List[int]]:\n",
    "    # Simple supervised format: compute loss only on the answer tokens\n",
    "    prompt = f\"Question: {q}\\nAnswer: \"\n",
    "    ans = a.strip()\n",
    "\n",
    "    prompt_ids = tokenizer(prompt, add_special_tokens=False).input_ids\n",
    "    answer_ids = tokenizer(ans + tokenizer.eos_token, add_special_tokens=False).input_ids\n",
    "\n",
    "    input_ids = prompt_ids + answer_ids\n",
    "    labels = [-100] * len(prompt_ids) + answer_ids.copy()\n",
    "\n",
    "    # Truncate to max_length\n",
    "    input_ids = input_ids[:max_length]\n",
    "    labels = labels[:max_length]\n",
    "    return {\"input_ids\": input_ids, \"labels\": labels}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7f96e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    args = parse_args()\n",
    "    set_seed(args.seed)\n",
    "\n",
    "    torch.backends.cuda.matmul.allow_tf32 = True\n",
    "\n",
    "    # Load tokenizer & base model (FP16). We do full-precision LoRA without 4/8-bit to keep DDP simple.\n",
    "    tokenizer = AutoTokenizer.from_pretrained(args.base_model, use_fast=True)\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        args.base_model,\n",
    "        torch_dtype=torch.float16,\n",
    "    )\n",
    "\n",
    "    if args.gradient_checkpointing:\n",
    "        model.gradient_checkpointing_enable()\n",
    "        model.config.use_cache = False  # important for gradient checkpointing\n",
    "\n",
    "    # Attach LoRA adapters\n",
    "    target_modules = [m.strip() for m in args.target_modules.split(\",\") if m.strip()]\n",
    "    lora_cfg = LoraConfig(\n",
    "        r=args.lora_r,\n",
    "        lora_alpha=args.lora_alpha,\n",
    "        lora_dropout=args.lora_dropout,\n",
    "        target_modules=target_modules,\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\",\n",
    "    )\n",
    "    model = get_peft_model(model, lora_cfg)\n",
    "    print_trainable_params(model)\n",
    "\n",
    "    # Load CSV and prepare dataset\n",
    "    if not os.path.exists(args.data_csv):\n",
    "        raise FileNotFoundError(f\"CSV not found: {args.data_csv}\")\n",
    "\n",
    "    raw = load_dataset(\"csv\", data_files={\"all\": args.data_csv})[\"all\"]\n",
    "\n",
    "    # Basic sanity checks\n",
    "    for col in [\"Question\", \"Answer\"]:\n",
    "        if col not in raw.column_names:\n",
    "            raise ValueError(f\"CSV must contain column: {col}\")\n",
    "\n",
    "    # Split train/validation\n",
    "    eval_ratio = max(0.001, min(0.5, args.eval_ratio))\n",
    "    split = raw.train_test_split(test_size=eval_ratio, seed=args.seed)\n",
    "    train_ds, eval_ds = split[\"train\"], split[\"test\"]\n",
    "\n",
    "    def _map_fn(example):\n",
    "        return build_prompt_and_labels(tokenizer, example[\"Question\"], example[\"Answer\"], args.max_length)\n",
    "\n",
    "    train_tok = train_ds.map(_map_fn, remove_columns=train_ds.column_names)\n",
    "    eval_tok = eval_ds.map(_map_fn, remove_columns=eval_ds.column_names)\n",
    "\n",
    "    data_collator = QADatasetCollator(tokenizer)\n",
    "\n",
    "    # Training setup\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=args.output_dir,\n",
    "        num_train_epochs=args.num_train_epochs,\n",
    "        per_device_train_batch_size=args.per_device_train_batch_size,\n",
    "        per_device_eval_batch_size=args.per_device_eval_batch_size,\n",
    "        gradient_accumulation_steps=args.gradient_accumulation_steps,\n",
    "        learning_rate=args.learning_rate,\n",
    "        warmup_ratio=args.warmup_ratio,\n",
    "        weight_decay=args.weight_decay,\n",
    "        logging_steps=args.logging_steps,\n",
    "        save_steps=args.save_steps,\n",
    "        save_total_limit=3,\n",
    "        evaluation_strategy=\"steps\",\n",
    "        eval_steps=args.save_steps,\n",
    "        fp16=True,\n",
    "        bf16=False,\n",
    "        gradient_checkpointing=args.gradient_checkpointing,\n",
    "        lr_scheduler_type=\"cosine\",\n",
    "        optim=\"adamw_torch\",\n",
    "        ddp_find_unused_parameters=False,\n",
    "        report_to=[\"none\"],  # set to [\"wandb\"] if you use Weights & Biases\n",
    "        dataloader_num_workers=2,\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_tok,\n",
    "        eval_dataset=eval_tok,\n",
    "        data_collator=data_collator,\n",
    "        tokenizer=tokenizer,\n",
    "    )\n",
    "\n",
    "    # Train (DDP will be active when launched with torchrun/accelerate across 2 GPUs)\n",
    "    trainer.train(resume_from_checkpoint=args.resume_from_checkpoint)\n",
    "\n",
    "    # Save final adapter and tokenizer\n",
    "    trainer.save_model(args.output_dir)\n",
    "    tokenizer.save_pretrained(args.output_dir)\n",
    "\n",
    "    if torch.distributed.is_initialized():\n",
    "        if torch.distributed.get_rank() == 0:\n",
    "            print(\"Training complete. Artifacts saved to:\", args.output_dir)\n",
    "    else:\n",
    "        print(\"Training complete. Artifacts saved to:\", args.output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1341c4c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9b1467b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a4cac76",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39f64f5c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
